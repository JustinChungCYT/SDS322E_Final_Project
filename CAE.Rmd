---
title: "Predicting Property Value"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F, echo = TRUE)
```

```{r, include=F}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(cluster)
library(reshape2)
library(GGally)
library(readxl)
library(ggpubr)
```

# Importing Dataset

```{r}
df <- read.csv("melbourne_housing.csv")
head(df, 10)
```

# Data Cleaning

```{r}
# Checking variables and their types
glimpse(df)

# Checking number of distinct values of each variable
df %>% summarize_all(n_distinct)

# Checking NAs
df %>% summarize_all(funs(sum(is.na(.))))
```
## Dealing with NA's

We first drop the rows with no Years built and no Building Area as that means there isn't a building on the property.
```{r}
df <- df %>% filter(!is.na(YearBuilt) & !is.na(BuildingArea))
head(df, 10)
```
We then change the NA's in `Car` as it means there is no car slot for the data.
```{r}
df <- df %>% mutate(Car = ifelse(is.na(Car), 0, Car))
head(df, 10)
```
Now we have a dataset with no NA's!

## Dropping useless columns 1
```{r}
df <- df %>% select(-c(Address, Suburb, Method, Postcode, Bedroom2, SellerG, Propertycount))
head(df, 10)
```
## Dropping useless columns 2
```{r}
df$YearSold <- as.numeric(format(as.Date(df$Date, format="%d/%m/%Y"),"%Y"))
df$MonthSold <- as.numeric(format(as.Date(df$Date, format="%d/%m/%Y"),"%m"))
df$MonthsSince2016 <- (df$YearSold-2016)*12 + df$MonthSold
df <- df %>% mutate(BuildingAge=YearSold-YearBuilt) %>% filter(BuildingAge >= 0)
df <- df %>% select(Price, Regionname, MonthsSince2016, BuildingAge, Landsize, BuildingArea, Lattitude, Longtitude, Type, Rooms, Distance, Bathroom, Car)
head(df, 10)
```
## Refactoring Features
```{r}
df <- df %>% transform(
  Price=as.double(Price),
  Regionname=as.factor(Regionname),
  MonthsSince2016=as.integer(MonthsSince2016),
  Landsize=as.double(Landsize),
  Type=as.factor(Type),
  Car=as.integer(Car)
)

head(df, 10)
```

## Dropping Unusual Data
```{r}
df <- df %>% filter(BuildingArea < 1250)
df <- df %>% filter(Price < 5000000)
```

## Exporting Cleaned Dataset
```{r}
write.csv(df, "cleaned_df.csv", row.names = F)
```

Now we're done cleaning!

# Exploratory Analysis

## Numerical Variables

### Summary and Scale Normalization
```{r}
num_df <- df %>% select(where(is.numeric))
summary(num_df)
```

### Correlation Matrix
```{r}
correlationmatrix_df = round(cor(num_df), 2)
correlationmatrix_df
```


### Correlation Heat Map
```{r}
get_lower_tri <- function(correlationmatrix_df) {
    correlationmatrix_df[upper.tri(correlationmatrix_df)] <- NA
    return(correlationmatrix_df)
}

lower_tri = get_lower_tri(correlationmatrix_df)

meltedcorrelationmatrix_df <- melt(lower_tri, na.rm = TRUE)
dfheatmap = ggplot(data = meltedcorrelationmatrix_df,
    aes(Var2, Var1, fill = value)) + geom_tile(color = "white") +
    scale_fill_gradient2(low = "blue", high = "red",
        mid = "white", space = "Lab", name = "Correlation") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1,
        size = 9, hjust = 1)) + coord_fixed() + geom_text(aes(Var2,
    Var1, label = value), color = "black", size = 2)

dfheatmap
```

### Price by Map
```{r}
ggplot(df, aes(x=Longtitude, y=Lattitude, col=Price)) + geom_point(size=0.5)
```
### Landsize
```{R}
p2 <- ggplot(df, aes(x=Landsize, y=Price))
p2 + geom_point(aes(color=Price)) + geom_smooth(method=lm) + stat_cor(method = "pearson")
```

### Building Area
```{R}
p3 <- ggplot(df, aes(x=BuildingArea, y=Price))
p3 + geom_point(aes(color=Price)) + geom_smooth(method=lm) + stat_cor(method = "pearson")
```

### Distance
```{R}
p3 <- ggplot(df, aes(x=Distance, y=Price))
p3 + geom_point(aes(color=Price)) + geom_smooth(method=lm) + stat_cor(method = "pearson")
```
### BuildingArea by Regionname
```{R, fig.height=10, fig.width=7}
p5 <- ggplot(df %>% filter(Regionname %in% c("Eastern Metropolitan", "Northern Metropolitan", "South-Eastern Metropolitan", "Southern Metropolitan", "Western Metropolitan")), aes(x=BuildingArea, y=Price, col=Regionname))
p5 + geom_point() + geom_smooth(method=lm) + stat_cor(method = "pearson") + facet_grid(rows=vars(Regionname))
```

### Distance by Regionname
```{R, fig.height=8, fig.width=7}
df_Regionname <- df %>% filter(Regionname=="Eastern Metropolitan" | Regionname=="Northern Metropolitan" | Regionname=="South-Eastern Metropolitan" | Regionname=="Western Metropolitan" | Regionname=="Eastern Victoria" | Regionname=="Northern Victoria") 
p6 <- ggplot(df_Regionname, aes(x=Distance, y=Price, color=Regionname))
p6 + geom_point() + geom_smooth(method=lm) + stat_cor(method = "pearson") + facet_grid(rows=vars(Regionname)) + scale_y_continuous(limits = c(0, 2500000))
```

# Hypothesis 1
The mean prices of properties are different based on number of rooms.
```{r}
summary(aov(Price ~ as.factor(Rooms), num_df))
```
With a One-Factor ANOVA Test, since the p-value is less than 0.001, we have significant evidence that the group mean price of properties are not the same based on different number of rooms. Therefore, we will include room count in our model as it can significantly impact the property price prediction.
```{r}
ggplot(df, aes(x=as.factor(Rooms), y=Price, fill=as.factor(Rooms))) + xlab("Rooms") + ylab("Price") + geom_bar(aes(y=Price), stat="summary", fun=mean) + geom_errorbar(stat="summary", width=0.5)
```

# Hypothesis 2
The mean prices of properties are different based on number of rooms.
```{r}
summary(aov(Price ~ Regionname, df))
```
With a One-Factor ANOVA Test, since the p-value is less than 0.001, we have significant evidence that the group mean price of properties are not the same based on different regions. Therefore, we will include region in our model as it can significantly impact the property price prediction.
```{r}
ggplot(df, aes(x=Regionname, y=Price, fill=Regionname)) + xlab("Regionname") + ylab("Price") + geom_bar(aes(y=Price), stat="summary", fun=mean) + geom_errorbar(stat="summary", width=0.5) + theme(axis.text.x = element_text(angle = 45, vjust=1, hjust=1))
```


# Hypothesis 3
The Price data is normally distributed.
```{r}
hist(df$Price)
qqnorm(df$Price); qqline(df$Price)
```

It's not normally distributed. We have to log-transform it.

```{r}
logPrice <- log(df$Price)
hist(logPrice)
qqnorm(logPrice); qqline(logPrice)
df$logPrice <- logPrice
```
After applying log transformation to the Price data, we are ready to put it into our model for prediction.



